# -*- coding: utf-8 -*-
"""Trabalho05.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1acOR_uc7MR9cRh3O3lCfWd52SL83xOCB
"""

#Lucca Libanori
'''Enunciado:
Sua tarefa será gerar a matriz termo documento, dos documentos recuperados da internet e 
imprimir esta matriz na tela. Para tanto:
a) Considere que todas as listas de sentenças devem ser transformadas em listas de vetores, 
onde cada item será uma das palavras da sentença.
b) Todos os vetores devem ser unidos em um corpus único formando uma lista de vetores, 
onde cada item será um lexema. 
c) Este único corpus será usado para gerar o vocabulário.
d) O resultado esperado será uma matriz termo documento criada a partir da aplicação da 
técnica bag of Words em todo o corpus.'''

from bs4 import BeautifulSoup
import requests
import string


url1 = ('https://levity.ai/blog/how-natural-language-processing-works')
url2 = ('https://en.wikipedia.org/wiki/Natural_language_processing')
url3 = ('https://www.datarobot.com/blog/what-is-natural-language-processing-introduction-to-nlp/')
url4 = ('https://www.qualtrics.com/experience-management/customer/natural-language-processing/')
url5 = ('https://www.oracle.com/hk/artificial-intelligence/what-is-natural-language-processing/')

urls = [url1, url2, url3, url4, url5]
sentencas = []
palavras = []
vocabulario = set()

for url in urls:
    sites = requests.get(url).text
    soup = BeautifulSoup(sites,'html.parser')

    for site in soup.find_all("p"):
        site = site.get_text().translate(str.maketrans('', '', string.punctuation))
        sentencas.append(site)
        
for soup in sentencas:
  palavras.append(soup.split())

soup = palavras

for site in soup:
  for palavra in site:
    vocabulario.add(palavra)

matriz = []
for sentenca in soup:
  vetor = []
  for palavra in vocabulario:
    if palavra in site:
      vetor.append(site.count(palavra))
    else:
      vetor.append(0)
  matriz.append(vetor)

print(matriz)